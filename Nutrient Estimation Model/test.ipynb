{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f55a0883-a7be-460c-a216-c4a08070260c",
   "metadata": {},
   "source": [
    "# Importing and Formatting data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a42fb35a-c186-4952-8d7c-9d4a8e840522",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df46c9ab-76e9-47d6-9932-ce2af2cde10c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dish</th>\n",
       "      <th>rgb_image</th>\n",
       "      <th>depth_image</th>\n",
       "      <th>ingr_id</th>\n",
       "      <th>ingr_name</th>\n",
       "      <th>grams</th>\n",
       "      <th>calories</th>\n",
       "      <th>fat</th>\n",
       "      <th>carb</th>\n",
       "      <th>protein</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dish_1559243887</td>\n",
       "      <td>b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\...</td>\n",
       "      <td>b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\...</td>\n",
       "      <td>ingr_0000000077</td>\n",
       "      <td>almonds</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>289.050000</td>\n",
       "      <td>24.750000</td>\n",
       "      <td>11.350000</td>\n",
       "      <td>11.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dish_1563568319</td>\n",
       "      <td>b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\...</td>\n",
       "      <td>b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\...</td>\n",
       "      <td>ingr_0000000026</td>\n",
       "      <td>white rice</td>\n",
       "      <td>14.128440</td>\n",
       "      <td>18.366972</td>\n",
       "      <td>0.042385</td>\n",
       "      <td>3.955963</td>\n",
       "      <td>0.381468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dish_1563568319</td>\n",
       "      <td>b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\...</td>\n",
       "      <td>b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\...</td>\n",
       "      <td>ingr_0000000046</td>\n",
       "      <td>mushroom</td>\n",
       "      <td>14.128440</td>\n",
       "      <td>3.108257</td>\n",
       "      <td>0.042385</td>\n",
       "      <td>0.466239</td>\n",
       "      <td>0.437982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dish_1563568319</td>\n",
       "      <td>b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\...</td>\n",
       "      <td>b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\...</td>\n",
       "      <td>ingr_0000000421</td>\n",
       "      <td>ground turkey</td>\n",
       "      <td>105.963303</td>\n",
       "      <td>273.385322</td>\n",
       "      <td>18.013762</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>26.490826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dish_1563568319</td>\n",
       "      <td>b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\...</td>\n",
       "      <td>b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\...</td>\n",
       "      <td>ingr_0000000161</td>\n",
       "      <td>olive oil</td>\n",
       "      <td>1.412844</td>\n",
       "      <td>12.489541</td>\n",
       "      <td>1.412844</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25023</th>\n",
       "      <td>dish_1566587689</td>\n",
       "      <td>b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\...</td>\n",
       "      <td>b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\...</td>\n",
       "      <td>ingr_0000000515</td>\n",
       "      <td>arugula</td>\n",
       "      <td>2.192604</td>\n",
       "      <td>0.548151</td>\n",
       "      <td>0.015348</td>\n",
       "      <td>0.081126</td>\n",
       "      <td>0.057008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25024</th>\n",
       "      <td>dish_1566587689</td>\n",
       "      <td>b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\...</td>\n",
       "      <td>b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\...</td>\n",
       "      <td>ingr_0000000312</td>\n",
       "      <td>pepper</td>\n",
       "      <td>2.740755</td>\n",
       "      <td>1.096302</td>\n",
       "      <td>0.005482</td>\n",
       "      <td>0.254890</td>\n",
       "      <td>0.054815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25025</th>\n",
       "      <td>dish_1575307819</td>\n",
       "      <td>b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\...</td>\n",
       "      <td>b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\...</td>\n",
       "      <td>ingr_0000000015</td>\n",
       "      <td>hash browns</td>\n",
       "      <td>114.000000</td>\n",
       "      <td>371.640000</td>\n",
       "      <td>25.080000</td>\n",
       "      <td>36.480000</td>\n",
       "      <td>2.964000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25026</th>\n",
       "      <td>dish_1559242155</td>\n",
       "      <td>b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\...</td>\n",
       "      <td>b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\...</td>\n",
       "      <td>ingr_0000000213</td>\n",
       "      <td>pears</td>\n",
       "      <td>203.000000</td>\n",
       "      <td>116.116000</td>\n",
       "      <td>0.203000</td>\n",
       "      <td>30.856000</td>\n",
       "      <td>0.812000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25027</th>\n",
       "      <td>dish_1562603536</td>\n",
       "      <td>b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\...</td>\n",
       "      <td>b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\...</td>\n",
       "      <td>ingr_0000000004</td>\n",
       "      <td>bacon</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>75.740000</td>\n",
       "      <td>5.880000</td>\n",
       "      <td>0.196000</td>\n",
       "      <td>5.180000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25028 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  dish                                          rgb_image  \\\n",
       "0      dish_1559243887  b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\...   \n",
       "1      dish_1563568319  b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\...   \n",
       "2      dish_1563568319  b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\...   \n",
       "3      dish_1563568319  b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\...   \n",
       "4      dish_1563568319  b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\...   \n",
       "...                ...                                                ...   \n",
       "25023  dish_1566587689  b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\...   \n",
       "25024  dish_1566587689  b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\...   \n",
       "25025  dish_1575307819  b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\...   \n",
       "25026  dish_1559242155  b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\...   \n",
       "25027  dish_1562603536  b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\...   \n",
       "\n",
       "                                             depth_image          ingr_id  \\\n",
       "0      b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\...  ingr_0000000077   \n",
       "1      b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\...  ingr_0000000026   \n",
       "2      b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\...  ingr_0000000046   \n",
       "3      b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\...  ingr_0000000421   \n",
       "4      b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\...  ingr_0000000161   \n",
       "...                                                  ...              ...   \n",
       "25023  b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\...  ingr_0000000515   \n",
       "25024  b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\...  ingr_0000000312   \n",
       "25025  b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\...  ingr_0000000015   \n",
       "25026  b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\...  ingr_0000000213   \n",
       "25027  b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\...  ingr_0000000004   \n",
       "\n",
       "           ingr_name       grams    calories        fat       carb    protein  \n",
       "0            almonds   50.000000  289.050000  24.750000  11.350000  11.200000  \n",
       "1         white rice   14.128440   18.366972   0.042385   3.955963   0.381468  \n",
       "2           mushroom   14.128440    3.108257   0.042385   0.466239   0.437982  \n",
       "3      ground turkey  105.963303  273.385322  18.013762   0.000000  26.490826  \n",
       "4          olive oil    1.412844   12.489541   1.412844   0.000000   0.000000  \n",
       "...              ...         ...         ...        ...        ...        ...  \n",
       "25023        arugula    2.192604    0.548151   0.015348   0.081126   0.057008  \n",
       "25024         pepper    2.740755    1.096302   0.005482   0.254890   0.054815  \n",
       "25025    hash browns  114.000000  371.640000  25.080000  36.480000   2.964000  \n",
       "25026          pears  203.000000  116.116000   0.203000  30.856000   0.812000  \n",
       "25027          bacon   14.000000   75.740000   5.880000   0.196000   5.180000  \n",
       "\n",
       "[25028 rows x 10 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nutrition5k_folder_path = r'E:\\My Drive\\Nutrition5kDataset'\n",
    "image_df = pd.read_pickle(os.path.join(nutrition5k_folder_path, 'dish_images.pkl'))\n",
    "# dishes = pd.read_excel(os.path.join(nutrition5k_folder_path, 'dishes.xlsx'))\n",
    "dish_ingredients = pd.read_excel(os.path.join(nutrition5k_folder_path, 'dish_ingredients.xlsx'))\n",
    "# ingredients = pd.read_excel(os.path.join(nutrition5k_folder_path, 'ingredients.xlsx'))\n",
    "nutrition5k_df = pd.merge(image_df, dish_ingredients, left_on='dish', right_on='dish_id', how='left').drop('dish_id', axis=1)\n",
    "nutrition5k_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dae92b79-8c17-4969-b0b2-17b54be90238",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ingredient_category</th>\n",
       "      <th>name</th>\n",
       "      <th>image_bytes</th>\n",
       "      <th>x1</th>\n",
       "      <th>y1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>rice</td>\n",
       "      <td>b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01...</td>\n",
       "      <td>0</td>\n",
       "      <td>143</td>\n",
       "      <td>370</td>\n",
       "      <td>486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>rice</td>\n",
       "      <td>b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x02\\x00...</td>\n",
       "      <td>71</td>\n",
       "      <td>16</td>\n",
       "      <td>478</td>\n",
       "      <td>328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>rice</td>\n",
       "      <td>b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01...</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>360</td>\n",
       "      <td>420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>rice</td>\n",
       "      <td>b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01...</td>\n",
       "      <td>19</td>\n",
       "      <td>69</td>\n",
       "      <td>355</td>\n",
       "      <td>372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>rice</td>\n",
       "      <td>b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>91</td>\n",
       "      <td>246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14356</th>\n",
       "      <td>100</td>\n",
       "      <td>goya chanpuru</td>\n",
       "      <td>b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00...</td>\n",
       "      <td>39</td>\n",
       "      <td>58</td>\n",
       "      <td>328</td>\n",
       "      <td>220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14357</th>\n",
       "      <td>100</td>\n",
       "      <td>goya chanpuru</td>\n",
       "      <td>b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01...</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>166</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14358</th>\n",
       "      <td>100</td>\n",
       "      <td>goya chanpuru</td>\n",
       "      <td>b'\\xff\\xd8\\xff\\xe1\\x03\\x88Exif\\x00\\x00II*\\x00\\...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>299</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14359</th>\n",
       "      <td>100</td>\n",
       "      <td>goya chanpuru</td>\n",
       "      <td>b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01...</td>\n",
       "      <td>456</td>\n",
       "      <td>125</td>\n",
       "      <td>800</td>\n",
       "      <td>351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14360</th>\n",
       "      <td>100</td>\n",
       "      <td>goya chanpuru</td>\n",
       "      <td>b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01...</td>\n",
       "      <td>246</td>\n",
       "      <td>30</td>\n",
       "      <td>590</td>\n",
       "      <td>245</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14361 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ingredient_category           name  \\\n",
       "0                        1           rice   \n",
       "1                        1           rice   \n",
       "2                        1           rice   \n",
       "3                        1           rice   \n",
       "4                        1           rice   \n",
       "...                    ...            ...   \n",
       "14356                  100  goya chanpuru   \n",
       "14357                  100  goya chanpuru   \n",
       "14358                  100  goya chanpuru   \n",
       "14359                  100  goya chanpuru   \n",
       "14360                  100  goya chanpuru   \n",
       "\n",
       "                                             image_bytes   x1   y1   x2   y2  \n",
       "0      b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01...    0  143  370  486  \n",
       "1      b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x02\\x00...   71   16  478  328  \n",
       "2      b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01...    0   49  360  420  \n",
       "3      b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01...   19   69  355  372  \n",
       "4      b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01...    0    4   91  246  \n",
       "...                                                  ...  ...  ...  ...  ...  \n",
       "14356  b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00...   39   58  328  220  \n",
       "14357  b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01...    4   11  166   92  \n",
       "14358  b'\\xff\\xd8\\xff\\xe1\\x03\\x88Exif\\x00\\x00II*\\x00\\...    0    0  299  175  \n",
       "14359  b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01...  456  125  800  351  \n",
       "14360  b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01...  246   30  590  245  \n",
       "\n",
       "[14361 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uecfood100_folder_path = r'D:\\Projects\\CDS\\Capstone Project\\UECFOOD100'\n",
    "uecfood100_df = pd.read_pickle(os.path.join(uecfood100_folder_path, 'images.pkl'))\n",
    "uecfood100_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "718c8d81-a166-4527-ba77-4f73a213c0ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rice': ['white rice', 'brown rice', 'wild rice', 'rice'],\n",
       " 'eels on rice': ['rice'],\n",
       " 'pilaf': ['wild rice', 'brown rice', 'white rice', 'rice'],\n",
       " \"chicken-'n'-egg on rice\": ['chicken', 'eggs', 'rice'],\n",
       " 'pork cutlet on rice': ['pork', 'bread'],\n",
       " 'beef curry': ['beef'],\n",
       " 'sushi': ['rice', 'fish', 'seaweed'],\n",
       " 'chicken rice': ['chicken', 'rice'],\n",
       " 'fried rice': ['fried rice', 'rice', 'eggs'],\n",
       " 'tempura bowl': ['rice', 'shrimp', 'squash', 'green beans', 'bell peppers'],\n",
       " 'bibimbap': ['rice',\n",
       "  'beef',\n",
       "  'spinach (raw)',\n",
       "  'bean sprouts',\n",
       "  'carrot',\n",
       "  'eggs'],\n",
       " 'toast': ['bread'],\n",
       " 'croissant': [],\n",
       " 'roll bread': ['bread', 'rolls'],\n",
       " 'raisin bread': ['raisin bread', 'bread'],\n",
       " 'chip butty': ['potato chips', 'bread', 'butter'],\n",
       " 'hamburger': ['ground beef',\n",
       "  'bread',\n",
       "  'lettuce',\n",
       "  'tomatoes',\n",
       "  'onions',\n",
       "  'cheese'],\n",
       " 'pizza': ['cheese pizza', 'pizza', 'pepperoni', 'mushroom', 'olives'],\n",
       " 'sandwiches': ['bread',\n",
       "  'ham',\n",
       "  'cheese',\n",
       "  'lettuce',\n",
       "  'tomatoes',\n",
       "  'mayonnaise',\n",
       "  'mustard'],\n",
       " 'udon noodle': ['noodles'],\n",
       " 'tempura udon': ['noodles',\n",
       "  'shrimp',\n",
       "  'squash',\n",
       "  'green beans',\n",
       "  'bell peppers'],\n",
       " 'soba noodle': ['noodles', 'seaweed'],\n",
       " 'ramen noodle': ['noodles', 'pork', 'eggs', 'seaweed', 'green onions'],\n",
       " 'beef noodle': ['noodles', 'beef'],\n",
       " 'tensin noodle': ['noodles', 'crab', 'eggs'],\n",
       " 'fried noodle': ['fried noodle',\n",
       "  'noodles',\n",
       "  'shrimp',\n",
       "  'cabbage',\n",
       "  'carrot',\n",
       "  'onions'],\n",
       " 'spaghetti': ['spaghetti', 'ground beef', 'cheese'],\n",
       " 'Japanese-style pancake': ['pancakes', 'cabbage', 'pork'],\n",
       " 'takoyaki': ['octopus', 'green onions'],\n",
       " 'gratin': ['pasta', 'potatoes', 'cheese', 'bread crumbs'],\n",
       " 'sauteed vegetables': ['carrot', 'broccoli', 'bell peppers', 'onions'],\n",
       " 'croquette': ['potatoes', 'ground beef', 'bread crumbs', 'eggs'],\n",
       " 'grilled eggplant': ['eggplant', 'oil'],\n",
       " 'sauteed spinach': ['spinach (raw)', 'garlic', 'oil'],\n",
       " 'vegetable tempura': ['broccoli', 'carrot', 'sweet potato', 'vegetable oil'],\n",
       " 'miso soup': ['tofu', 'seaweed', 'green onions'],\n",
       " 'potage': ['potatoes', 'leeks', 'carrot', 'cream'],\n",
       " 'sausage': ['sausage'],\n",
       " 'oden': ['eggs', 'soy sauce'],\n",
       " 'omelet': ['omelets', 'eggs'],\n",
       " 'ganmodoki': ['tofu', 'carrot', 'seaweed', 'soy sauce'],\n",
       " 'jiaozi': ['ground pork', 'cabbage', 'chive', 'ginger', 'soy sauce'],\n",
       " 'stew': ['beef', 'potatoes', 'carrots', 'celery'],\n",
       " 'teriyaki grilled fish': ['fish'],\n",
       " 'fried fish': ['fish', 'flour', 'oil'],\n",
       " 'grilled salmon': ['salmon', 'oil', 'lemon'],\n",
       " 'salmon meuniere ': ['salmon', 'butter', 'flour', 'lemon'],\n",
       " 'sashimi': ['fish'],\n",
       " 'grilled pacific saury ': ['fish', 'soy sauce'],\n",
       " 'sukiyaki': ['beef', 'tofu', 'cabbage', 'mushroom', 'onions', 'soy sauce'],\n",
       " 'sweet and sour pork': ['pork', 'bell peppers', 'onions', 'pineapple'],\n",
       " 'lightly roasted fish': ['fish'],\n",
       " 'steamed egg hotchpotch': ['eggs', 'chicken', 'shrimp', 'vegetables'],\n",
       " 'tempura': ['shrimp',\n",
       "  'sweet potato',\n",
       "  'eggplant',\n",
       "  'green beans',\n",
       "  'vegetable oil'],\n",
       " 'fried chicken': ['fried chicken', 'chicken', 'flour', 'oil'],\n",
       " 'sirloin cutlet ': ['beef', 'bread crumbs', 'eggs', 'oil'],\n",
       " 'nanbanzuke': ['fish', 'vinegar', 'onions', 'carrots', 'bell peppers'],\n",
       " 'boiled fish': ['fish', 'soy sauce', 'ginger'],\n",
       " 'seasoned beef with potatoes': ['beef', 'potatoes', 'onions', 'soy sauce'],\n",
       " 'hambarg steak': ['ground beef', 'bread crumbs', 'eggs', 'onions'],\n",
       " 'beef steak': ['steak', 'beef'],\n",
       " 'dried fish': ['fish'],\n",
       " 'ginger pork saute': ['pork', 'ginger', 'soy sauce', 'onions'],\n",
       " 'spicy chili-flavored tofu': ['tofu', 'chili', 'sesame oil', 'soy sauce'],\n",
       " 'yakitori': ['chicken', 'green onions'],\n",
       " 'cabbage roll': ['cabbage', 'ground pork', 'rice', 'tomato'],\n",
       " 'rolled omelet': ['eggs', 'soy sauce'],\n",
       " 'egg sunny-side up': ['eggs'],\n",
       " 'fermented soybeans': [],\n",
       " 'cold tofu': ['tofu', 'soy sauce', 'green onions', 'ginger'],\n",
       " 'egg roll': ['cabbage', 'carrot', 'pork', 'shrimp'],\n",
       " 'chilled noodle': ['noodles',\n",
       "  'soy sauce',\n",
       "  'cucumber',\n",
       "  'tomatoes',\n",
       "  'sesame seeds'],\n",
       " 'stir-fried beef and peppers': ['beef',\n",
       "  'bell peppers',\n",
       "  'onions',\n",
       "  'soy sauce'],\n",
       " 'simmered pork': ['pork', 'soy sauce', 'ginger'],\n",
       " 'boiled chicken and vegetables': ['chicken',\n",
       "  'carrots',\n",
       "  'potatoes',\n",
       "  'onions',\n",
       "  'celery'],\n",
       " 'sashimi bowl': ['rice', 'fish', 'seaweed', 'soy sauce'],\n",
       " 'sushi bowl': ['rice', 'fish', 'eggs', 'vegetables', 'seaweed'],\n",
       " 'fish-shaped pancake with bean jam': ['pancakes'],\n",
       " 'shrimp with chill source': ['shrimp', 'chili', 'garlic', 'ginger'],\n",
       " 'roast chicken': ['chicken', 'rosemary', 'thyme', 'garlic', 'onions'],\n",
       " 'steamed meat dumpling': ['ground pork', 'cabbage', 'ginger', 'soy sauce'],\n",
       " 'omelet with fried rice': ['eggs', 'rice', 'peas', 'carrot', 'green onions'],\n",
       " 'cutlet curry': ['pork', 'bread crumbs', 'egg', 'rice'],\n",
       " 'spaghetti meat sauce': ['pasta',\n",
       "  'ground beef',\n",
       "  'tomato',\n",
       "  'onions',\n",
       "  'garlic',\n",
       "  'herbs'],\n",
       " 'fried shrimp': ['shrimp', 'flour', 'eggs', 'bread crumbs', 'oil'],\n",
       " 'potato salad': ['potatoes',\n",
       "  'mayonnaise',\n",
       "  'celery',\n",
       "  'onions',\n",
       "  'mustard',\n",
       "  'eggs'],\n",
       " 'green salad': ['lettuce',\n",
       "  'tomatoes',\n",
       "  'cucumbers',\n",
       "  'carrots',\n",
       "  'salad dressing'],\n",
       " 'macaroni salad': ['macaroni',\n",
       "  'mayonnaise',\n",
       "  'celery',\n",
       "  'onions',\n",
       "  'bell peppers',\n",
       "  'eggs'],\n",
       " 'Japanese tofu and vegetable chowder': ['tofu', 'fish', 'miso', 'vegetables'],\n",
       " 'pork miso soup': ['miso', 'fish', 'pork', 'onions', 'tofu'],\n",
       " 'chinese soup': ['vegetables', 'meat', 'shrimp', 'soy sauce'],\n",
       " 'beef bowl': ['rice', 'beef', 'onions', 'soy sauce'],\n",
       " 'kinpira-style sauteed burdock': ['carrot', 'soy sauce', 'sesame oil'],\n",
       " 'rice ball': ['rice', 'seaweed', 'tuna', 'pickles'],\n",
       " 'pizza toast': ['bread', 'tomato', 'cheese', 'pepperoni', 'mushroom'],\n",
       " 'dipping noodles': ['noodles', 'soy sauce', 'green onions', 'seaweed'],\n",
       " 'hot dog': ['hot dogs', 'bread', 'mustard', 'ketchup', 'onions'],\n",
       " 'french fries': ['potatoes', 'oil', 'salt'],\n",
       " 'mixed rice': ['rice', 'vegetables', 'bean(seed) ', 'meat'],\n",
       " 'goya chanpuru': ['tofu', 'eggs', 'pork', 'vegetables']}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from uecfood100_ingredient_mapping import ingredient_mapping\n",
    "ingredient_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82e1ca14-d2e4-406d-91fc-9f564f25d6e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UECFood100 formatted shape: (14361, 6)\n",
      "Nutrition5k formatted shape: (3490, 6)\n",
      "Final dataset shape: (17851, 6)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# -------------- UECFOOD100 Formatting --------------\n",
    "\n",
    "# Build global ingredient list\n",
    "all_ingredients = set()\n",
    "for ingredients in ingredient_mapping.values():\n",
    "    all_ingredients.update(ingredients)\n",
    "all_ingredients = sorted(list(all_ingredients))  # fixed order\n",
    "ingredient_to_index = {ingredient: idx for idx, ingredient in enumerate(all_ingredients)}\n",
    "\n",
    "# Function to create multi-hot vector\n",
    "def create_multihot_vector_uec(dish_name):\n",
    "    multihot = [0] * len(all_ingredients)\n",
    "    ingredients = ingredient_mapping.get(dish_name, [])\n",
    "    for ing in ingredients:\n",
    "        if ing in ingredient_to_index:\n",
    "            multihot[ingredient_to_index[ing]] = 1\n",
    "    return multihot\n",
    "\n",
    "# Add multi-hot vector column\n",
    "uecfood100_df['ingredient_multihot'] = uecfood100_df['name'].apply(create_multihot_vector_uec)\n",
    "\n",
    "# UEC Final Format\n",
    "uecfood100_formatted = pd.DataFrame({\n",
    "    'image': uecfood100_df['image_bytes'],\n",
    "    'ingredients': uecfood100_df['ingredient_multihot'],\n",
    "    'calories': np.nan,\n",
    "    'fat': np.nan,\n",
    "    'carb': np.nan,\n",
    "    'protein': np.nan\n",
    "})\n",
    "\n",
    "print(f\"UECFood100 formatted shape: {uecfood100_formatted.shape}\")\n",
    "\n",
    "# -------------- Nutrition5k Formatting --------------\n",
    "\n",
    "# Build global ingredient list for Nutrition5k\n",
    "nutrition5k_ingredients = nutrition5k_df['ingr_name'].dropna().unique().tolist()\n",
    "nutrition5k_ingredients = sorted(list(set(nutrition5k_ingredients + all_ingredients)))  # Merge with UEC ones\n",
    "\n",
    "ingredient_to_index_nutrition5k = {ingredient: idx for idx, ingredient in enumerate(nutrition5k_ingredients)}\n",
    "\n",
    "# Group ingredients per dish\n",
    "dish_groups = nutrition5k_df.groupby('dish')\n",
    "\n",
    "# Function to create multi-hot vector\n",
    "def create_multihot_vector_nutrition5k(ingredients):\n",
    "    multihot = [0] * len(nutrition5k_ingredients)\n",
    "    for ing in ingredients:\n",
    "        if ing in ingredient_to_index_nutrition5k:\n",
    "            multihot[ingredient_to_index_nutrition5k[ing]] = 1\n",
    "    return multihot\n",
    "\n",
    "# Build Nutrition5k formatted dataframe\n",
    "nutrition5k_formatted_list = []\n",
    "\n",
    "for dish_id, group in dish_groups:\n",
    "    image_bytes = group['rgb_image'].iloc[0]  # get one image per dish\n",
    "    ingredients = group['ingr_name'].tolist()\n",
    "    total_calories = group['calories'].sum()\n",
    "    total_fat = group['fat'].sum()\n",
    "    total_carb = group['carb'].sum()\n",
    "    total_protein = group['protein'].sum()\n",
    "    \n",
    "    multihot = create_multihot_vector_nutrition5k(ingredients)\n",
    "    \n",
    "    nutrition5k_formatted_list.append({\n",
    "        'image': image_bytes,\n",
    "        'ingredients': multihot,\n",
    "        'calories': total_calories,\n",
    "        'fat': total_fat,\n",
    "        'carb': total_carb,\n",
    "        'protein': total_protein\n",
    "    })\n",
    "\n",
    "nutrition5k_formatted = pd.DataFrame(nutrition5k_formatted_list)\n",
    "\n",
    "print(f\"Nutrition5k formatted shape: {nutrition5k_formatted.shape}\")\n",
    "\n",
    "# -------------- Merge Both Datasets --------------\n",
    "\n",
    "# Important: Make sure same ingredient list length\n",
    "# We will extend UEC ingredient vectors to match Nutrition5k vector length\n",
    "\n",
    "def pad_multihot_vector(vector, final_length):\n",
    "    if len(vector) < final_length:\n",
    "        vector = vector + [0]*(final_length - len(vector))\n",
    "    return vector\n",
    "\n",
    "# Pad UEC multihot vectors\n",
    "uecfood100_formatted['ingredients'] = uecfood100_formatted['ingredients'].apply(lambda x: pad_multihot_vector(x, len(nutrition5k_ingredients)))\n",
    "\n",
    "# Merge both\n",
    "final_dataset = pd.concat([uecfood100_formatted, nutrition5k_formatted], ignore_index=True)\n",
    "final_ingredients = nutrition5k_ingredients\n",
    "\n",
    "print(f\"Final dataset shape: {final_dataset.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a10260af-0200-4736-94d0-d4cff6280ba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>calories</th>\n",
       "      <th>fat</th>\n",
       "      <th>carb</th>\n",
       "      <th>protein</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x02\\x00...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17846</th>\n",
       "      <td>b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>189.350000</td>\n",
       "      <td>14.700000</td>\n",
       "      <td>0.490000</td>\n",
       "      <td>12.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17847</th>\n",
       "      <td>b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>359.811659</td>\n",
       "      <td>26.935999</td>\n",
       "      <td>3.726333</td>\n",
       "      <td>24.365833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17848</th>\n",
       "      <td>b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>682.551659</td>\n",
       "      <td>48.715999</td>\n",
       "      <td>35.406333</td>\n",
       "      <td>26.939833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17849</th>\n",
       "      <td>b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>267.960000</td>\n",
       "      <td>20.358000</td>\n",
       "      <td>1.044000</td>\n",
       "      <td>18.096000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17850</th>\n",
       "      <td>b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>362.500000</td>\n",
       "      <td>26.738000</td>\n",
       "      <td>10.324000</td>\n",
       "      <td>18.850000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17851 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   image  \\\n",
       "0      b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01...   \n",
       "1      b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x02\\x00...   \n",
       "2      b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01...   \n",
       "3      b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01...   \n",
       "4      b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01...   \n",
       "...                                                  ...   \n",
       "17846  b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\...   \n",
       "17847  b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\...   \n",
       "17848  b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\...   \n",
       "17849  b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\...   \n",
       "17850  b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\...   \n",
       "\n",
       "                                             ingredients    calories  \\\n",
       "0      [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...         NaN   \n",
       "1      [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...         NaN   \n",
       "2      [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...         NaN   \n",
       "3      [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...         NaN   \n",
       "4      [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...         NaN   \n",
       "...                                                  ...         ...   \n",
       "17846  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...  189.350000   \n",
       "17847  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...  359.811659   \n",
       "17848  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...  682.551659   \n",
       "17849  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  267.960000   \n",
       "17850  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  362.500000   \n",
       "\n",
       "             fat       carb    protein  \n",
       "0            NaN        NaN        NaN  \n",
       "1            NaN        NaN        NaN  \n",
       "2            NaN        NaN        NaN  \n",
       "3            NaN        NaN        NaN  \n",
       "4            NaN        NaN        NaN  \n",
       "...          ...        ...        ...  \n",
       "17846  14.700000   0.490000  12.950000  \n",
       "17847  26.935999   3.726333  24.365833  \n",
       "17848  48.715999  35.406333  26.939833  \n",
       "17849  20.358000   1.044000  18.096000  \n",
       "17850  26.738000  10.324000  18.850000  \n",
       "\n",
       "[17851 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ee9917-7db9-43cb-a9e9-d1784db9835d",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "894e10cb-7909-4564-b54c-cc170f53768b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch.utils.data import Dataset\n",
    "# from PIL import Image\n",
    "# import io\n",
    "# import time\n",
    "\n",
    "# class FoodDataset(Dataset):\n",
    "#     def __init__(self, dataframe):\n",
    "#         self.df = dataframe\n",
    "#         self.ingredient_dim = len(self.df.iloc[0]['ingredients'])\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.df)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         row = self.df.iloc[idx]\n",
    "        \n",
    "#         # Decode image\n",
    "#         image = Image.open(io.BytesIO(row['image'])).convert('RGB')\n",
    "        \n",
    "#         # Convert image to tensor\n",
    "#         image = self.transform(image)\n",
    "\n",
    "#         # Ingredients multi-hot\n",
    "#         ingredients = torch.FloatTensor(row['ingredients'])\n",
    "\n",
    "#         # Nutrients\n",
    "#         nutrients = {\n",
    "#             'calories': row['calories'],\n",
    "#             'fat': row['fat'],\n",
    "#             'carb': row['carb'],\n",
    "#             'protein': row['protein']\n",
    "#         }\n",
    "\n",
    "#         return image, ingredients, nutrients\n",
    "\n",
    "#     def __getitem__debug(self, idx):\n",
    "#         print('in function')\n",
    "#         start_time = time.time()\n",
    "\n",
    "#         row = self.df.iloc[idx]\n",
    "#         print('got row')\n",
    "        \n",
    "#         # Decode image\n",
    "#         image = Image.open(io.BytesIO(row['image'])).convert('RGB')\n",
    "#         print(f\"Image Load Time: {time.time() - start_time} seconds\")\n",
    "\n",
    "#         start_time = time.time()\n",
    "#         # Convert image to tensor\n",
    "#         image = self.transform(image).float()\n",
    "#         print(f\"Image Transform Time: {time.time() - start_time} seconds\")\n",
    "\n",
    "#         start_time = time.time()\n",
    "#         # Ingredients multi-hot\n",
    "#         ingredients = torch.FloatTensor(row['ingredients']).float()\n",
    "#         print(f\"Ingredients Encoding Time: {time.time() - start_time} seconds\")\n",
    "\n",
    "#         start_time = time.time()\n",
    "#         # Nutrients\n",
    "#         nutrients = {\n",
    "#             'calories': row['calories'],\n",
    "#             'fat': row['fat'],\n",
    "#             'carb': row['carb'],\n",
    "#             'protein': row['protein']\n",
    "#         }\n",
    "#         print(f\"Nutrients Encoding Time: {time.time() - start_time} seconds\")\n",
    "\n",
    "#         return image, ingredients, nutrients\n",
    "\n",
    "#     @staticmethod\n",
    "#     def transform(image):\n",
    "#         from torchvision import transforms\n",
    "#         transform_ops = transforms.Compose([\n",
    "#             transforms.Resize((224, 224)),\n",
    "#             transforms.ToTensor(),  # Converts to [0,1]\n",
    "#             transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # Imagenet stats\n",
    "#         ])\n",
    "#         return transform_ops(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea9eda58-2576-40a5-929b-d77441b6a8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# train_dataset = FoodDataset(final_dataset)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=0)\n",
    "\n",
    "# # Test: check a batch\n",
    "# for images, ingredients, nutrients in tqdm(train_loader, desc=\"Training Batches\", ncols=100):\n",
    "#     print(images.shape)        # [B, 3, 224, 224]\n",
    "#     print(ingredients.shape)   # [B, num_ingredients]\n",
    "#     print(nutrients)\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69dba2d1-7413-48ff-9e4d-eaf539590c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from torch.utils.data import DataLoader\n",
    "# import torchvision\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # Model Definition\n",
    "# class FoodModel(nn.Module):\n",
    "#     # def __init__(self, num_ingredients):\n",
    "#     #     super(FoodModel, self).__init__()\n",
    "#     #     # Pretrained ResNet as backbone\n",
    "#     #     self.backbone = torchvision.models.resnet18(pretrained=True)\n",
    "#     #     self.backbone.fc = nn.Identity()  # Removing the fully connected layer to use features\n",
    "\n",
    "#     #     # Multi-label classification head for ingredients\n",
    "#     #     self.ingredients_head = nn.Sequential(\n",
    "#     #         nn.Linear(self.backbone.fc.in_features, 512),\n",
    "#     #         nn.ReLU(),\n",
    "#     #         nn.Dropout(0.5),\n",
    "#     #         nn.Linear(512, num_ingredients),  # num_ingredients output units\n",
    "#     #         nn.Sigmoid()  # Sigmoid for multi-label classification (probability for each ingredient)\n",
    "#     #     )\n",
    "\n",
    "#     #     # Regression head for nutritional values\n",
    "#     #     self.nutrients_head = nn.Sequential(\n",
    "#     #         nn.Linear(self.backbone.fc.in_features, 512),\n",
    "#     #         nn.ReLU(),\n",
    "#     #         nn.Dropout(0.5),\n",
    "#     #         nn.Linear(512, 4)  # 4 outputs: calories, fat, carbs, protein\n",
    "#     #     )\n",
    "\n",
    "#     def __init__(self, num_ingredients):\n",
    "#         super(FoodModel, self).__init__()\n",
    "#         # Pretrained ResNet as backbone\n",
    "#         self.backbone = torchvision.models.resnet18(pretrained=True)\n",
    "#         # Removing the fully connected layer to use features\n",
    "#         self.backbone.fc = nn.Identity()  \n",
    "\n",
    "#         # Number of output features from ResNet backbone (ResNet18)\n",
    "#         resnet_output_features = 512  # This is the size of the feature map output from ResNet\n",
    "\n",
    "#         # Multi-label classification head for ingredients\n",
    "#         self.ingredients_head = nn.Sequential(\n",
    "#             nn.Linear(resnet_output_features, 512),  # Use resnet_output_features instead of in_features\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.5),\n",
    "#             nn.Linear(512, num_ingredients),  # num_ingredients output units\n",
    "#             nn.Sigmoid()  # Sigmoid for multi-label classification (probability for each ingredient)\n",
    "#         )\n",
    "\n",
    "#         # Regression head for nutritional values\n",
    "#         self.nutrients_head = nn.Sequential(\n",
    "#             nn.Linear(resnet_output_features, 512),  # Use resnet_output_features instead of in_features\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.5),\n",
    "#             nn.Linear(512, 4)  # 4 outputs: calories, fat, carbs, protein\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         features = self.backbone(x)\n",
    "\n",
    "#         # Multi-label classification (ingredients prediction)\n",
    "#         ingredients = self.ingredients_head(features)\n",
    "\n",
    "#         # Nutritional regression (calories, fat, carbs, protein)\n",
    "#         nutrients = self.nutrients_head(features)\n",
    "\n",
    "#         return ingredients, nutrients\n",
    "\n",
    "# # Training Loop\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# def train_model(model, train_loader, epochs=10, lr=1e-4, device='cpu'):\n",
    "#     model.to(device)\n",
    "#     criterion_ingredients = nn.BCELoss()  # Binary Cross-Entropy for multi-label classification\n",
    "#     criterion_nutrients = nn.MSELoss()  # Mean Squared Error for regression\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "#     for epoch in range(epochs):\n",
    "#         model.train()  # Set model to training mode\n",
    "#         running_loss = 0.0\n",
    "#         correct_preds = 0\n",
    "#         total_preds = 0\n",
    "\n",
    "#         for images, ingredients, nutrients in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\", ncols=100):\n",
    "#             # Move data to the correct device (GPU/CPU)\n",
    "#             images = images.to(device).float()  # Ensure float32\n",
    "#             ingredients = ingredients.to(device).float()  # Ensure float32\n",
    "            \n",
    "#             # Move each element in nutrients dictionary to the correct device\n",
    "#             nutrients = {key: value.to(device).float() for key, value in nutrients.items()}\n",
    "\n",
    "#             # Zero gradients\n",
    "#             optimizer.zero_grad()\n",
    "\n",
    "#             # Forward pass\n",
    "#             predicted_ingredients, predicted_nutrients = model(images)\n",
    "\n",
    "#             print(predicted_ingredients)\n",
    "\n",
    "#             # Apply sigmoid to the predicted ingredients\n",
    "#             predicted_ingredients = torch.sigmoid(predicted_ingredients)  # Sigmoid to get probabilities\n",
    "\n",
    "#             print(predicted_ingredients)\n",
    "#             assert torch.all(predicted_ingredients >= 0) and torch.all(predicted_ingredients <= 1), \\\n",
    "#                 \"Predicted ingredients are not in the range [0, 1]\"\n",
    "\n",
    "#             # Calculate losses for ingredients\n",
    "#             loss_ingredients = criterion_ingredients(predicted_ingredients, ingredients)\n",
    "\n",
    "#             # Assuming predicted_nutrients is a tensor of shape [B, 4] for [calories, fat, carbs, protein]\n",
    "#             # Calculate loss for nutrients (e.g., calories)\n",
    "#             loss_nutrients = criterion_nutrients(predicted_nutrients[:, 0], nutrients['calories'])\n",
    "\n",
    "#             # Total loss\n",
    "#             loss = loss_ingredients + loss_nutrients\n",
    "\n",
    "#             # Backpropagation\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#             running_loss += loss.item()\n",
    "\n",
    "#             # Calculate accuracy for ingredients (optional)\n",
    "#             predicted_labels = (predicted_ingredients > 0.5).float()\n",
    "#             correct_preds += (predicted_labels == ingredients).sum().item()\n",
    "#             total_preds += ingredients.numel()\n",
    "\n",
    "#         avg_loss = running_loss / len(train_loader)\n",
    "#         accuracy = correct_preds / total_preds\n",
    "\n",
    "#         print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Initialize the model\n",
    "# num_ingredients = len(final_ingredients)  \n",
    "# model = FoodModel(num_ingredients)\n",
    "\n",
    "# # Assuming `train_loader` is your DataLoader for the dataset\n",
    "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)  # Set num_workers to 0 as per your system\n",
    "\n",
    "# # Start training\n",
    "# train_model(model, train_loader, epochs=10, lr=1e-4, device='cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b592e35d-4688-4b5f-9eea-8a59fd79afa5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d105d4-1153-4caf-876e-a4a9b4e937b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea9f6135-9d00-492a-8e6e-3672220a6cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import random_split\n",
    "\n",
    "# # Train/Test/Validation Split\n",
    "# def split_data(dataset, val_split=0.1, test_split=0.1):\n",
    "#     total_size = len(dataset)\n",
    "#     val_size = int(total_size * val_split)\n",
    "#     test_size = int(total_size * test_split)\n",
    "#     train_size = total_size - val_size - test_size\n",
    "\n",
    "#     train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "#     return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "# # Assuming dataset is already defined as final_dataset\n",
    "# train_dataset, val_dataset, test_dataset = split_data(final_dataset)\n",
    "\n",
    "# # Data Loaders\n",
    "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1abd02f-d402-408f-b426-20b1d06fad21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "# import torchvision.models as models\n",
    "\n",
    "# class FoodMultiTaskModel(nn.Module):\n",
    "#     def __init__(self, ingredient_dim):\n",
    "#         super(FoodMultiTaskModel, self).__init__()\n",
    "        \n",
    "#         # Backbone\n",
    "#         self.backbone = models.resnet18(pretrained=True)\n",
    "#         in_features = self.backbone.fc.in_features\n",
    "#         self.backbone.fc = nn.Identity()  # Remove original classifier\n",
    "\n",
    "#         # Heads\n",
    "#         self.ingredient_head = nn.Sequential(\n",
    "#             nn.Linear(in_features, 512),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(512, ingredient_dim)\n",
    "#         )\n",
    "        \n",
    "#         self.nutrient_head = nn.Sequential(\n",
    "#             nn.Linear(in_features, 512),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(512, 4)  # calories, fat, carb, protein\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         features = self.backbone(x)\n",
    "#         ingredients = self.ingredient_head(features)\n",
    "#         nutrients = self.nutrient_head(features)\n",
    "#         return ingredients, nutrients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a810d025-f404-48fd-9faa-d69c7588e1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.optim as optim\n",
    "# from torch.utils.data import DataLoader\n",
    "# import torch.nn.functional as F\n",
    "# from tqdm import tqdm  # Import tqdm for progress bar\n",
    "\n",
    "# # Assuming dataset and dataloader are already defined\n",
    "# # Assuming FoodMultiTaskModel is already defined\n",
    "\n",
    "# # Loss Functions\n",
    "# criterion_ingredients = nn.BCEWithLogitsLoss()  # Binary cross entropy for multi-class ingredient prediction\n",
    "# criterion_nutrients = nn.MSELoss()  # Mean squared error for nutrient prediction\n",
    "\n",
    "# # Device\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# # Training Loop\n",
    "# def train_model(model, train_loader, epochs, lr, device):\n",
    "#     model.train()\n",
    "    \n",
    "#     for epoch in range(epochs):\n",
    "#         running_loss = 0.0\n",
    "#         # Wrap the train_loader with tqdm for progress bar\n",
    "#         for images, ingredients, nutrients in tqdm(train_loader, desc=f'Batch {epoch+1}/{epochs}', unit='batch'):\n",
    "#             # Move data to device (GPU or CPU)\n",
    "#             images = images.to(device)\n",
    "#             ingredients = ingredients.to(device)\n",
    "#             nutrients = {key: value.to(device).float() for key, value in nutrients.items()}\n",
    "            \n",
    "#             # Forward pass\n",
    "#             predicted_ingredients, predicted_nutrients = model(images)\n",
    "            \n",
    "#             # Calculate losses\n",
    "#             loss_ingredients = criterion_ingredients(predicted_ingredients, ingredients)\n",
    "#             loss_nutrients = criterion_nutrients(predicted_nutrients[:, 0], nutrients['calories'])\n",
    "            \n",
    "#             # Total loss\n",
    "#             loss = loss_ingredients + loss_nutrients\n",
    "            \n",
    "#             # Backpropagation\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "            \n",
    "#             running_loss += loss.item()\n",
    "\n",
    "#         print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "# # Model, optimizer\n",
    "# num_ingredients = len(final_ingredients)  \n",
    "# model = FoodMultiTaskModel(ingredient_dim=num_ingredients).to(device)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# # Assuming train_loader is defined\n",
    "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "\n",
    "# # Train the model\n",
    "# train_model(model, train_loader, epochs=10, lr=1e-4, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a88249-4d92-4d65-97e3-316ac61c854b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9239d3e5-9ab0-4407-9ae4-1b4be56f85e6",
   "metadata": {},
   "source": [
    "# Model Training V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d080bc9c-1e2e-408d-9107-1281cd3f502a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 17851\n",
      "Labeled samples (Nutrition5k): 3490\n",
      "Unlabeled samples (UECFood100): 14361\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Add the 'is_labeled' column\n",
    "final_dataset['is_labeled'] = ~final_dataset['calories'].isna()\n",
    "\n",
    "# Display counts to verify\n",
    "print(f\"Total samples: {len(final_dataset)}\")\n",
    "print(f\"Labeled samples (Nutrition5k): {final_dataset['is_labeled'].sum()}\")\n",
    "print(f\"Unlabeled samples (UECFood100): {len(final_dataset) - final_dataset['is_labeled'].sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9bb5cff8-59e6-46f1-8aa8-9e195a542177",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# Example Transforms (using ImageNet normalization)\n",
    "IMG_SIZE = 224\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "# Weak augmentations (for supervised loss, evaluation, and one branch of SSL)\n",
    "weak_transforms = transforms.Compose([\n",
    "    # Assuming input is PIL Image after loading/decoding bytes\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(IMG_SIZE),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "\n",
    "# Strong augmentations (for the other branch of SSL consistency)\n",
    "strong_transforms = transforms.Compose([\n",
    "    # Assuming input is PIL Image\n",
    "    transforms.Resize(256),\n",
    "    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.3, 1.0)), # More aggressive crop\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandAugment(num_ops=2, magnitude=10), # Example: RandAugment\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "    # transforms.RandomErasing(p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False) # Example: Cutout/Erasing\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a6accef-2d9e-46e8-b320-5b1fce9711c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "class SemiSupervisedFoodDataset(Dataset):\n",
    "    def __init__(self, dataframe, weak_transform, strong_transform):\n",
    "        self.dataframe = dataframe\n",
    "        self.weak_transform = weak_transform\n",
    "        self.strong_transform = strong_transform\n",
    "\n",
    "        # Pre-process columns for efficiency if needed, or do it in __getitem__\n",
    "        # Ensure correct data types before converting to tensors\n",
    "        self.dataframe['ingredients'] = self.dataframe['ingredients'].apply(lambda x: np.array(x, dtype=np.float32))\n",
    "        nut_cols = ['calories', 'fat', 'carb', 'protein']\n",
    "        # Fill NaNs with a placeholder (e.g., 0) temporarily for tensor conversion,\n",
    "        # the 'is_labeled' flag will tell us which ones are valid\n",
    "        self.dataframe[nut_cols] = self.dataframe[nut_cols].fillna(0).astype(np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "\n",
    "        image_bytes = row['image']\n",
    "\n",
    "        # --- Ensure Ingredients are correct type ---\n",
    "        # Add similar check/conversion for ingredients just in case\n",
    "        ingredients_np = row['ingredients']\n",
    "        if not isinstance(ingredients_np, np.ndarray) or ingredients_np.dtype != np.float32:\n",
    "            # If ingredients weren't pre-processed correctly in __init__ or are list\n",
    "             ingredients_np = np.array(ingredients_np, dtype=np.float32)\n",
    "        ingredients = torch.tensor(ingredients_np, dtype=torch.float32) # Use the numpy array\n",
    "\n",
    "        # --- Fix for Nutrients ---\n",
    "        nut_cols = ['calories', 'fat', 'carb', 'protein']\n",
    "        # 1. Select the slice\n",
    "        # 2. Fill NaNs (using fillna on the Series slice)\n",
    "        # 3. Explicitly convert to float32 using astype\n",
    "        # 4. Get the numpy .values\n",
    "        nut_values_np = row[nut_cols].astype(np.float32).fillna(0).astype(np.float32).values\n",
    "\n",
    "        # 5. Now create the tensor from the correctly typed numpy array\n",
    "        nutrients = torch.tensor(nut_values_np, dtype=torch.float32) # Should work now\n",
    "\n",
    "        is_labeled_np = np.array([row['is_labeled']], dtype=bool)\n",
    "        is_labeled = torch.tensor(is_labeled_np, dtype=torch.bool)\n",
    "\n",
    "        # --- Decode Image Bytes & Apply Transforms (as before) ---\n",
    "        try:\n",
    "            image = Image.open(io.BytesIO(image_bytes)).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image at index {idx}: {e}\")\n",
    "            image = Image.new('RGB', (IMG_SIZE, IMG_SIZE), color = 'red') # Placeholder\n",
    "\n",
    "        if is_labeled:\n",
    "            img_transformed = self.weak_transform(image)\n",
    "            # For labeled data, return weak, weak (as strong isn't needed but collate expects 2)\n",
    "            # Or adjust collate_fn later\n",
    "            return img_transformed, img_transformed, ingredients, nutrients, is_labeled\n",
    "        else:\n",
    "            img_weak = self.weak_transform(image)\n",
    "            img_strong = self.strong_transform(image)\n",
    "            # Return nutrients (filled with 0s, but marked as unlabeled)\n",
    "            return img_weak, img_strong, ingredients, nutrients, is_labeled\n",
    "\n",
    "# --- Create a separate Dataset for Evaluation ---\n",
    "# Uses only weak transforms and expects single image output\n",
    "class EvaluationFoodDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform):\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "        # Pre-processing similar to SemiSupervisedFoodDataset\n",
    "        self.dataframe['ingredients'] = self.dataframe['ingredients'].apply(lambda x: np.array(x, dtype=np.float32))\n",
    "        nut_cols = ['calories', 'fat', 'carb', 'protein']\n",
    "        self.dataframe[nut_cols] = self.dataframe[nut_cols].fillna(0).astype(np.float32) # Fill NaNs for tensor creation\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        image_bytes = row['image']\n",
    "        ingredients = torch.tensor(row['ingredients'], dtype=torch.float32)\n",
    "        nutrients = torch.tensor(row[['calories', 'fat', 'carb', 'protein']].values, dtype=torch.float32)\n",
    "        is_labeled = torch.tensor(row['is_labeled'], dtype=torch.bool) # Keep flag for potential filtering\n",
    "\n",
    "        try:\n",
    "            image = Image.open(io.BytesIO(image_bytes)).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image at index {idx} during eval: {e}\")\n",
    "            image = Image.new('RGB', (IMG_SIZE, IMG_SIZE), color = 'blue') # Placeholder\n",
    "\n",
    "        img_transformed = self.transform(image)\n",
    "        return img_transformed, ingredients, nutrients, is_labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "befc7a4b-e410-4f89-8e0d-340efed2a7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split, Subset, DataLoader\n",
    "\n",
    "# Define split sizes\n",
    "total_size = len(final_dataset)\n",
    "val_split = 0.1\n",
    "test_split = 0.1\n",
    "val_size = int(total_size * val_split)\n",
    "test_size = int(total_size * test_split)\n",
    "train_size = total_size - val_size - test_size\n",
    "\n",
    "# Generate indices and split them\n",
    "indices = list(range(total_size))\n",
    "np.random.shuffle(indices)\n",
    "train_indices = indices[:train_size]\n",
    "val_indices = indices[train_size : train_size + val_size]\n",
    "test_indices = indices[train_size + val_size :]\n",
    "\n",
    "# Create Dataset instances using the full dataframe\n",
    "# Note: Creating DataFrames for subsets might be memory intensive if large\n",
    "# Using Subsets is generally preferred. Pass the whole dataframe.\n",
    "train_df_subset = final_dataset.iloc[train_indices].reset_index(drop=True)\n",
    "val_df_subset = final_dataset.iloc[val_indices].reset_index(drop=True)\n",
    "test_df_subset = final_dataset.iloc[test_indices].reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Create the SSL dataset for training\n",
    "train_dataset = SemiSupervisedFoodDataset(train_df_subset, weak_transforms, strong_transforms)\n",
    "\n",
    "# Create evaluation datasets for validation and testing\n",
    "val_dataset = EvaluationFoodDataset(val_df_subset, weak_transforms) # Only weak transforms\n",
    "test_dataset = EvaluationFoodDataset(test_df_subset, weak_transforms) # Only weak transforms\n",
    "\n",
    "\n",
    "# Create DataLoaders\n",
    "BATCH_SIZE = 32 # Adjust as needed\n",
    "NUM_WORKERS = 0 # Adjust based on your system\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36ba674b-f5fb-4d36-9d54-7285407b354d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchvision.models import ResNet18_Weights\n",
    "\n",
    "class FoodMultiTaskModel(nn.Module):\n",
    "    def __init__(self, ingredient_dim):\n",
    "        super(FoodMultiTaskModel, self).__init__()\n",
    "        \n",
    "        # Backbone\n",
    "        self.backbone = models.resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "        in_features = self.backbone.fc.in_features\n",
    "        self.backbone.fc = nn.Identity()  # Remove original classifier\n",
    "\n",
    "        # Heads\n",
    "        self.ingredient_head = nn.Sequential(\n",
    "            nn.Linear(in_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, ingredient_dim)\n",
    "        )\n",
    "        \n",
    "        self.nutrient_head = nn.Sequential(\n",
    "            nn.Linear(in_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 4)  # calories, fat, carb, protein\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        ingredients = self.ingredient_head(features)\n",
    "        nutrients = self.nutrient_head(features)\n",
    "        return ingredients, nutrients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6be49acf-0aad-4b56-9801-e95e990db863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "\n",
    "# # Assuming your FoodMultiTaskModel is defined as before\n",
    "# ingredient_dim = len(final_ingredients) # Get the final ingredient dimension\n",
    "# model = FoodMultiTaskModel(ingredient_dim=ingredient_dim)\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "\n",
    "# # Loss Functions\n",
    "# criterion_ingredients = nn.BCEWithLogitsLoss() # Multi-label classification\n",
    "# criterion_nutrients_sup = nn.MSELoss()       # Regression for labeled data\n",
    "# criterion_nutrients_unsup = nn.MSELoss()     # Consistency loss (e.g., MSE)\n",
    "\n",
    "# # Optimizer\n",
    "# optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# # Hyperparameter for unsupervised loss weight\n",
    "# lambda_unsup = 1.0 # Start with 1.0, tune as needed. Consider ramping up.\n",
    "\n",
    "# # --- Training Loop Example ---\n",
    "# num_epochs = 20 # Set number of epochs\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     running_loss = 0.0\n",
    "#     running_loss_ing = 0.0\n",
    "#     running_loss_nut_sup = 0.0\n",
    "#     running_loss_nut_unsup = 0.0\n",
    "\n",
    "#     for batch_idx, data in enumerate(train_loader):\n",
    "#         # Data unpacking depends on how __getitem__ returns it\n",
    "#         # Assuming: img_data, ingredients_target, nutrients_target, is_labeled = data\n",
    "#         img_data, ingredients_target, nutrients_target, is_labeled = data\n",
    "\n",
    "#         ingredients_target = ingredients_target.to(device)\n",
    "#         nutrients_target = nutrients_target.to(device)\n",
    "#         is_labeled = is_labeled.to(device)\n",
    "\n",
    "#         # Separate labeled and unlabeled based on the flag\n",
    "#         labeled_mask = is_labeled\n",
    "#         unlabeled_mask = ~is_labeled\n",
    "\n",
    "#         loss_ing_total = 0\n",
    "#         loss_nut_sup = 0\n",
    "#         loss_nut_unsup = 0\n",
    "\n",
    "#         # --- Process Labeled Data ---\n",
    "#         if labeled_mask.any():\n",
    "#             # img_data for labeled is a single tensor\n",
    "#             img_labeled = img_data[labeled_mask].to(device)\n",
    "#             ing_target_labeled = ingredients_target[labeled_mask]\n",
    "#             nut_target_labeled = nutrients_target[labeled_mask]\n",
    "\n",
    "#             pred_ing_labeled, pred_nut_labeled = model(img_labeled)\n",
    "\n",
    "#             loss_ing_labeled = criterion_ingredients(pred_ing_labeled, ing_target_labeled)\n",
    "#             loss_nut_sup = criterion_nutrients_sup(pred_nut_labeled, nut_target_labeled)\n",
    "#             loss_ing_total += loss_ing_labeled * img_labeled.size(0) # Weighted by batch size\n",
    "\n",
    "#         # --- Process Unlabeled Data ---\n",
    "#         if unlabeled_mask.any():\n",
    "#             # img_data for unlabeled is a tuple (img_weak, img_strong)\n",
    "#             # Need to handle this separation carefully. Let's adjust the loader logic or collate_fn if needed.\n",
    "#             # Assuming img_data structure might need adjustment or a custom collate:\n",
    "#             # For simplicity, let's assume the dataset __getitem__ was adjusted to return\n",
    "#             # img_weak, img_strong, ingredients, nutrients, is_labeled always,\n",
    "#             # where img_strong might be None or same as img_weak for labeled.\n",
    "\n",
    "#             # --- Revised Data Loading Assumption (Simpler Collation) ---\n",
    "#             # Let's assume __getitem__ always returns: img_weak, img_strong, ingredients, nutrients, is_labeled\n",
    "#             # Where for labeled: img_strong = img_weak (or None, handled later)\n",
    "\n",
    "#             # img_weak, img_strong = img_data # This assumes img_data structure from __getitem__\n",
    "#             # Need to filter based on mask *after* getting predictions\n",
    "\n",
    "#             img_weak_all = img_data[0].to(device) # Assuming img_data[0] is weak view\n",
    "#             img_strong_all = img_data[1].to(device) # Assuming img_data[1] is strong view\n",
    "\n",
    "#             # Predict ingredients using weak augmentation for all (consistency with supervised)\n",
    "#             pred_ing_weak_all, _ = model(img_weak_all) # Ignore nutrient prediction here\n",
    "\n",
    "#             # Calculate ingredient loss for unlabeled using their targets\n",
    "#             ing_target_unlabeled = ingredients_target[unlabeled_mask]\n",
    "#             pred_ing_unlabeled = pred_ing_weak_all[unlabeled_mask]\n",
    "#             loss_ing_unlabeled = criterion_ingredients(pred_ing_unlabeled, ing_target_unlabeled)\n",
    "#             loss_ing_total += loss_ing_unlabeled * img_weak_all[unlabeled_mask].size(0)\n",
    "\n",
    "#             # Predict nutrients using both weak and strong augmentations for unlabeled\n",
    "#             _, pred_nut_weak_unlabeled = model(img_weak_all[unlabeled_mask])\n",
    "#             _, pred_nut_strong_unlabeled = model(img_strong_all[unlabeled_mask])\n",
    "\n",
    "#             # Calculate unsupervised consistency loss\n",
    "#             loss_nut_unsup = criterion_nutrients_unsup(\n",
    "#                 pred_nut_strong_unlabeled,\n",
    "#                 pred_nut_weak_unlabeled.detach() # Detach target to prevent gradients clash\n",
    "#             )\n",
    "\n",
    "#         # --- Combine Losses ---\n",
    "#         # Average ingredient loss over the entire batch size processed for ingredients\n",
    "#         num_ing_samples = labeled_mask.sum() + unlabeled_mask.sum()\n",
    "#         if num_ing_samples > 0:\n",
    "#              loss_ing = loss_ing_total / num_ing_samples\n",
    "#         else:\n",
    "#              loss_ing = 0\n",
    "\n",
    "#         # If no labeled samples, loss_nut_sup is 0\n",
    "#         # If no unlabeled samples, loss_nut_unsup is 0\n",
    "#         # Handle potential division by zero if a batch has only one type\n",
    "\n",
    "#         total_loss = loss_ing # Start with ingredient loss (applied to all)\n",
    "\n",
    "#         if labeled_mask.any():\n",
    "#             total_loss += loss_nut_sup # Add supervised nutrient loss\n",
    "\n",
    "#         if unlabeled_mask.any():\n",
    "#             total_loss += lambda_unsup * loss_nut_unsup # Add weighted unsupervised nutrient loss\n",
    "\n",
    "#         # Backpropagation\n",
    "#         optimizer.zero_grad()\n",
    "#         if isinstance(total_loss, torch.Tensor): # Ensure loss is computed\n",
    "#             total_loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#             # Accumulate running losses for reporting\n",
    "#             running_loss += total_loss.item()\n",
    "#             if isinstance(loss_ing, torch.Tensor): running_loss_ing += loss_ing.item()\n",
    "#             if isinstance(loss_nut_sup, torch.Tensor): running_loss_nut_sup += loss_nut_sup.item()\n",
    "#             if isinstance(loss_nut_unsup, torch.Tensor): running_loss_nut_unsup += loss_nut_unsup.item()\n",
    "#         else:\n",
    "#             # Handle cases where a batch might have no valid samples for any loss component\n",
    "#              pass\n",
    "\n",
    "\n",
    "#         if batch_idx % 100 == 99: # Print stats every 100 batches\n",
    "#              print(f'[Epoch {epoch + 1}, Batch {batch_idx + 1}] Loss: {running_loss / 100:.4f} '\n",
    "#                    f'| Ing: {running_loss_ing / 100:.4f} '\n",
    "#                    f'| Nut Sup: {running_loss_nut_sup / 100:.4f} '\n",
    "#                    f'| Nut Unsup: {running_loss_nut_unsup / 100:.4f}')\n",
    "#              running_loss = 0.0\n",
    "#              running_loss_ing = 0.0\n",
    "#              running_loss_nut_sup = 0.0\n",
    "#              running_loss_nut_unsup = 0.0\n",
    "\n",
    "#     # --- Validation Step ---\n",
    "#     model.eval()\n",
    "#     # ... (Implement validation using val_loader and EvaluationFoodDataset)\n",
    "#     # Calculate validation metrics for ingredients and nutrients (only on labeled val samples)\n",
    "\n",
    "# print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f2d8d702-a79d-4c5c-8398-babdd7e7fc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# import torch\n",
    "\n",
    "# # Assuming your FoodMultiTaskModel is defined as before\n",
    "# # Assuming final_ingredients is defined\n",
    "# ingredient_dim = len(final_ingredients) # Get the final ingredient dimension\n",
    "# model = FoodMultiTaskModel(ingredient_dim=ingredient_dim)\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "\n",
    "# # Loss Functions\n",
    "# criterion_ingredients = nn.BCEWithLogitsLoss() # Multi-label classification\n",
    "# criterion_nutrients_sup = nn.MSELoss()         # Regression for labeled data\n",
    "# criterion_nutrients_unsup = nn.MSELoss()         # Consistency loss (e.g., MSE)\n",
    "\n",
    "# # Optimizer\n",
    "# optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# # Hyperparameter for unsupervised loss weight\n",
    "# lambda_unsup = 1.0 # Start with 1.0, tune as needed. Consider ramping up.\n",
    "\n",
    "# # --- Training Loop Example ---\n",
    "# num_epochs = 20 # Set number of epochs\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     running_loss = 0.0\n",
    "#     running_loss_ing = 0.0\n",
    "#     running_loss_nut_sup = 0.0\n",
    "#     running_loss_nut_unsup = 0.0\n",
    "\n",
    "#     for batch_idx, data in enumerate(train_loader):\n",
    "#         # --- Revised Data Unpacking ---\n",
    "#         if len(data) == 4:\n",
    "#             img_data, ingredients_target, nutrients_target, is_labeled = data\n",
    "#             img_weak_all = img_data.to(device)\n",
    "#             img_strong_all = img_data.to(device) # For labeled, weak and strong are the same\n",
    "#         elif len(data) == 5:\n",
    "#             img_weak_all, img_strong_all, ingredients_target, nutrients_target, is_labeled = data\n",
    "#             img_weak_all = img_weak_all.to(device)\n",
    "#             img_strong_all = img_strong_all.to(device)\n",
    "#         else:\n",
    "#             raise ValueError(f\"Unexpected number of elements in data: {len(data)}\")\n",
    "#         # --- End Revised Data Unpacking ---\n",
    "\n",
    "#         ingredients_target = ingredients_target.to(device)\n",
    "#         nutrients_target = nutrients_target.to(device)\n",
    "#         is_labeled = is_labeled.to(device)\n",
    "\n",
    "#         # Separate labeled and unlabeled based on the flag\n",
    "#         labeled_mask = is_labeled\n",
    "#         unlabeled_mask = ~is_labeled\n",
    "\n",
    "#         loss_ing_total = 0\n",
    "#         loss_nut_sup = 0\n",
    "#         loss_nut_unsup = 0\n",
    "\n",
    "#         # --- Process Labeled Data ---\n",
    "#         if labeled_mask.any():\n",
    "#             img_labeled = img_weak_all[labeled_mask] # Use weak augmentation for consistency\n",
    "#             ing_target_labeled = ingredients_target[labeled_mask]\n",
    "#             nut_target_labeled = nutrients_target[labeled_mask]\n",
    "\n",
    "#             pred_ing_labeled, pred_nut_labeled = model(img_labeled)\n",
    "\n",
    "#             loss_ing_labeled = criterion_ingredients(pred_ing_labeled, ing_target_labeled)\n",
    "#             loss_nut_sup = criterion_nutrients_sup(pred_nut_labeled, nut_target_labeled)\n",
    "#             loss_ing_total += loss_ing_labeled * img_labeled.size(0) # Weighted by batch size\n",
    "\n",
    "#         # --- Process Unlabeled Data ---\n",
    "#         if unlabeled_mask.any():\n",
    "#             img_weak_unlabeled = img_weak_all[unlabeled_mask]\n",
    "#             img_strong_unlabeled = img_strong_all[unlabeled_mask]\n",
    "#             ing_target_unlabeled = ingredients_target[unlabeled_mask] # These targets might not be meaningful for unsupervised loss\n",
    "\n",
    "#             # Predict ingredients using weak augmentation\n",
    "#             pred_ing_weak_unlabeled, _ = model(img_weak_unlabeled)\n",
    "#             loss_ing_unlabeled = criterion_ingredients(pred_ing_weak_unlabeled, ing_target_unlabeled) # Still calculate for potential regularization\n",
    "#             loss_ing_total += loss_ing_unlabeled * img_weak_unlabeled.size(0)\n",
    "\n",
    "#             # Predict nutrients using both weak and strong augmentations\n",
    "#             _, pred_nut_weak_unlabeled = model(img_weak_unlabeled)\n",
    "#             _, pred_nut_strong_unlabeled = model(img_strong_unlabeled)\n",
    "\n",
    "#             # Calculate unsupervised consistency loss\n",
    "#             loss_nut_unsup = criterion_nutrients_unsup(\n",
    "#                 pred_nut_strong_unlabeled,\n",
    "#                 pred_nut_weak_unlabeled.detach() # Detach target to prevent gradients clash\n",
    "#             )\n",
    "\n",
    "#         # --- Combine Losses ---\n",
    "#         # Average ingredient loss over the entire batch size processed for ingredients\n",
    "#         num_ing_samples = labeled_mask.sum() + unlabeled_mask.sum()\n",
    "#         if num_ing_samples > 0:\n",
    "#             loss_ing = loss_ing_total / num_ing_samples\n",
    "#         else:\n",
    "#             loss_ing = 0\n",
    "\n",
    "#         total_loss = loss_ing # Start with ingredient loss (applied to all)\n",
    "\n",
    "#         if labeled_mask.any():\n",
    "#             total_loss += loss_nut_sup # Add supervised nutrient loss\n",
    "\n",
    "#         if unlabeled_mask.any():\n",
    "#             total_loss += lambda_unsup * loss_nut_unsup # Add weighted unsupervised nutrient loss\n",
    "\n",
    "#         # Backpropagation\n",
    "#         optimizer.zero_grad()\n",
    "#         if isinstance(total_loss, torch.Tensor): # Ensure loss is computed\n",
    "#             total_loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#             # Accumulate running losses for reporting\n",
    "#             running_loss += total_loss.item()\n",
    "#             if isinstance(loss_ing, torch.Tensor): running_loss_ing += loss_ing.item()\n",
    "#             if isinstance(loss_nut_sup, torch.Tensor): running_loss_nut_sup += loss_nut_sup.item()\n",
    "#             if isinstance(loss_nut_unsup, torch.Tensor): running_loss_nut_unsup += loss_nut_unsup.item()\n",
    "#         else:\n",
    "#             # Handle cases where a batch might have no valid samples for any loss component\n",
    "#             pass\n",
    "\n",
    "\n",
    "#         if batch_idx % 100 == 99: # Print stats every 100 batches\n",
    "#             print(f'[Epoch {epoch + 1}, Batch {batch_idx + 1}] Loss: {running_loss / 100:.4f} '\n",
    "#                   f'| Ing: {running_loss_ing / 100:.4f} '\n",
    "#                   f'| Nut Sup: {running_loss_nut_sup / 100:.4f} '\n",
    "#                   f'| Nut Unsup: {running_loss_nut_unsup / 100:.4f}')\n",
    "#             running_loss = 0.0\n",
    "#             running_loss_ing = 0.0\n",
    "#             running_loss_nut_sup = 0.0\n",
    "#             running_loss_nut_unsup = 0.0\n",
    "\n",
    "#     # --- Validation Step ---\n",
    "#     model.eval()\n",
    "#     # ... (Implement validation using val_loader and EvaluationFoodDataset)\n",
    "#     # Calculate validation metrics for ingredients and nutrients (only on labeled val samples)\n",
    "\n",
    "# print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "26301b3f-f4aa-43ca-9844-c5d33401a7b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20:  12%| | 55/447 [03:27<24:35,  3.76s/it, ing=22093.7320, loss=22093.7320, nut_sup=22088.2968, nut_unsup=5.07\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 88\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;66;03m# Predict nutrients using both weak and strong augmentations\u001b[39;00m\n\u001b[0;32m     87\u001b[0m _, pred_nut_weak_unlabeled \u001b[38;5;241m=\u001b[39m model(img_weak_unlabeled)\n\u001b[1;32m---> 88\u001b[0m _, pred_nut_strong_unlabeled \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_strong_unlabeled\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;66;03m# Calculate unsupervised consistency loss\u001b[39;00m\n\u001b[0;32m     91\u001b[0m loss_nut_unsup \u001b[38;5;241m=\u001b[39m criterion_nutrients_unsup(\n\u001b[0;32m     92\u001b[0m     pred_nut_strong_unlabeled,\n\u001b[0;32m     93\u001b[0m     pred_nut_weak_unlabeled\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;66;03m# Detach target to prevent gradients clash\u001b[39;00m\n\u001b[0;32m     94\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\jupyterlab\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\jupyterlab\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[17], line 28\u001b[0m, in \u001b[0;36mFoodMultiTaskModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 28\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m     ingredients \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mingredient_head(features)\n\u001b[0;32m     30\u001b[0m     nutrients \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnutrient_head(features)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\jupyterlab\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\jupyterlab\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\jupyterlab\\lib\\site-packages\\torchvision\\models\\resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\jupyterlab\\lib\\site-packages\\torchvision\\models\\resnet.py:276\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    274\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(x)\n\u001b[0;32m    275\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer3(x)\n\u001b[1;32m--> 276\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer4\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    278\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavgpool(x)\n\u001b[0;32m    279\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(x, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\jupyterlab\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\jupyterlab\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\jupyterlab\\lib\\site-packages\\torch\\nn\\modules\\container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\jupyterlab\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\jupyterlab\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\jupyterlab\\lib\\site-packages\\torchvision\\models\\resnet.py:96\u001b[0m, in \u001b[0;36mBasicBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     93\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(out)\n\u001b[0;32m     94\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n\u001b[1;32m---> 96\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     97\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2(out)\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\jupyterlab\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\jupyterlab\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\jupyterlab\\lib\\site-packages\\torch\\nn\\modules\\conv.py:458\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\jupyterlab\\lib\\site-packages\\torch\\nn\\modules\\conv.py:454\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    452\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    453\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming your FoodMultiTaskModel is defined as before\n",
    "# Assuming final_ingredients is defined\n",
    "ingredient_dim = len(final_ingredients) # Get the final ingredient dimension\n",
    "model = FoodMultiTaskModel(ingredient_dim=ingredient_dim)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Loss Functions\n",
    "criterion_ingredients = nn.BCEWithLogitsLoss() # Multi-label classification\n",
    "criterion_nutrients_sup = nn.MSELoss()\n",
    "criterion_nutrients_unsup = nn.MSELoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Hyperparameter for unsupervised loss weight\n",
    "lambda_unsup = 1.0 # Start with 1.0, tune as needed. Consider ramping up.\n",
    "\n",
    "# --- Training Loop Example ---\n",
    "num_epochs = 20 # Set number of epochs\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_loss_ing = 0.0\n",
    "    running_loss_nut_sup = 0.0\n",
    "    running_loss_nut_unsup = 0.0\n",
    "\n",
    "    # Wrap the train_loader with tqdm for a progress bar\n",
    "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "    for batch_idx, data in progress_bar:\n",
    "        # --- Revised Data Unpacking ---\n",
    "        if len(data) == 4:\n",
    "            img_data, ingredients_target, nutrients_target, is_labeled = data\n",
    "            img_weak_all = img_data.to(device)\n",
    "            img_strong_all = img_data.to(device) # For labeled, weak and strong are the same\n",
    "        elif len(data) == 5:\n",
    "            img_weak_all, img_strong_all, ingredients_target, nutrients_target, is_labeled = data\n",
    "            img_weak_all = img_weak_all.to(device)\n",
    "            img_strong_all = img_strong_all.to(device)\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected number of elements in data: {len(data)}\")\n",
    "        # --- End Revised Data Unpacking ---\n",
    "\n",
    "        ingredients_target = ingredients_target.to(device)\n",
    "        nutrients_target = nutrients_target.to(device)\n",
    "        is_labeled = is_labeled.to(device)\n",
    "\n",
    "        # Separate labeled and unlabeled based on the flag\n",
    "        labeled_mask = is_labeled.squeeze()\n",
    "        unlabeled_mask = (~is_labeled).squeeze()\n",
    "\n",
    "        loss_ing_total = 0\n",
    "        loss_nut_sup = 0\n",
    "        loss_nut_unsup = 0\n",
    "\n",
    "        # --- Process Labeled Data ---\n",
    "        if labeled_mask.any():\n",
    "            img_labeled = img_weak_all[labeled_mask] # Use weak augmentation for consistency\n",
    "            ing_target_labeled = ingredients_target[labeled_mask]\n",
    "            nut_target_labeled = nutrients_target[labeled_mask]\n",
    "\n",
    "            pred_ing_labeled, pred_nut_labeled = model(img_labeled)\n",
    "\n",
    "            loss_ing_labeled = criterion_ingredients(pred_ing_labeled, ing_target_labeled)\n",
    "            loss_nut_sup = criterion_nutrients_sup(pred_nut_labeled, nut_target_labeled)\n",
    "            loss_ing_total += loss_ing_labeled * img_labeled.size(0) # Weighted by batch size\n",
    "\n",
    "        # --- Process Unlabeled Data ---\n",
    "        if unlabeled_mask.any():\n",
    "            img_weak_unlabeled = img_weak_all[unlabeled_mask]\n",
    "            img_strong_unlabeled = img_strong_all[unlabeled_mask]\n",
    "            ing_target_unlabeled = ingredients_target[unlabeled_mask] # These targets might not be meaningful for unsupervised loss\n",
    "\n",
    "            # Predict ingredients using weak augmentation\n",
    "            pred_ing_weak_unlabeled, _ = model(img_weak_unlabeled)\n",
    "            loss_ing_unlabeled = criterion_ingredients(pred_ing_weak_unlabeled, ing_target_unlabeled) # Still calculate for potential regularization\n",
    "            loss_ing_total += loss_ing_unlabeled * img_weak_unlabeled.size(0)\n",
    "\n",
    "            # Predict nutrients using both weak and strong augmentations\n",
    "            _, pred_nut_weak_unlabeled = model(img_weak_unlabeled)\n",
    "            _, pred_nut_strong_unlabeled = model(img_strong_unlabeled)\n",
    "\n",
    "            # Calculate unsupervised consistency loss\n",
    "            loss_nut_unsup = criterion_nutrients_unsup(\n",
    "                pred_nut_strong_unlabeled,\n",
    "                pred_nut_weak_unlabeled.detach() # Detach target to prevent gradients clash\n",
    "            )\n",
    "\n",
    "        # --- Combine Losses ---\n",
    "        # Average ingredient loss over the entire batch size processed for ingredients\n",
    "        num_ing_samples = labeled_mask.sum() + unlabeled_mask.sum()\n",
    "        if num_ing_samples > 0:\n",
    "            loss_ing = loss_ing_total / num_ing_samples\n",
    "        else:\n",
    "            loss_ing = 0\n",
    "\n",
    "        total_loss = loss_ing # Start with ingredient loss (applied to all)\n",
    "\n",
    "        if labeled_mask.any():\n",
    "            total_loss += loss_nut_sup # Add supervised nutrient loss\n",
    "\n",
    "        if unlabeled_mask.any():\n",
    "            total_loss += lambda_unsup * loss_nut_unsup # Add weighted unsupervised nutrient loss\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        if isinstance(total_loss, torch.Tensor): # Ensure loss is computed\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate running losses for reporting within the tqdm bar\n",
    "            running_loss += total_loss.item()\n",
    "            running_loss_ing += loss_ing.item() if isinstance(loss_ing, torch.Tensor) else 0.0\n",
    "            running_loss_nut_sup += loss_nut_sup.item() if isinstance(loss_nut_sup, torch.Tensor) else 0.0\n",
    "            running_loss_nut_unsup += loss_nut_unsup.item() if isinstance(loss_nut_unsup, torch.Tensor) else 0.0\n",
    "\n",
    "            # Update the tqdm progress bar with current loss values\n",
    "            progress_bar.set_postfix(loss=f\"{running_loss / (batch_idx + 1):.4f}\",\n",
    "                                     ing=f\"{running_loss_ing / (batch_idx + 1):.4f}\",\n",
    "                                     nut_sup=f\"{running_loss_nut_sup / (batch_idx + 1):.4f}\",\n",
    "                                     nut_unsup=f\"{running_loss_nut_unsup / (batch_idx + 1):.4f}\")\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    # --- Validation Step ---\n",
    "    model.eval()\n",
    "    # ... (Implement validation using val_loader and EvaluationFoodDataset)\n",
    "    # Calculate validation metrics for ingredients and nutrients (only on labeled val samples)\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cf7b8a-9f1b-47fd-8c44-e8716b2e125e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
